<img src="https://user-images.githubusercontent.com/100426366/218293510-3282e2e5-9f13-4664-b1d5-2db02719f192.png" width=20%>

# Unmute   
## ðŸ”¹Problem Statement
To develop a system that can accurately detect and interpret sign language gestures in real-time and provide a corresponding text output for people who are deaf or hard of hearing.
The goal is to provide an inclusive and accessible platform for communication and improve the quality of life for people who are deaf or hard of hearing.
<br>
## ðŸ”¹SOLUTION
Our solution is a innovative web portal that uses a machine learning model to predict the letter being signed in each frame of the video. It collects the stream of predicted letters and uses a filtering algorithm to extract the text being signed.
We also aim to provide a library which will have a collection of images of signs corresponding to letters and words.
<br>
## ðŸ”¹Site Map
The contents in this project follow the following structure:

```
â””â”€â”€â”€Home
    |â”€â”€ Sign Language detector and predictor screen
â””â”€â”€â”€ Library to learn Sign Language

```
